{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import httpx\n",
    "\n",
    "# Define the endpoint for the containerized API\n",
    "API_URL = \"http://localhost:8000/inference\"  # Update the URL and endpoint as needed\n",
    "\n",
    "# Example data to send in the POST request\n",
    "PAYLOAD = {\"input\": \"This is a test input for the model.\"}\n",
    "\n",
    "# Number of parallel requests\n",
    "NUM_REQUESTS = 10\n",
    "\n",
    "\n",
    "async def send_request(client: httpx.AsyncClient, request_id: int):\n",
    "    \"\"\"Send a single POST request to the containerized API and print the response.\"\"\"\n",
    "    try:\n",
    "        response = await client.post(API_URL, json=PAYLOAD)\n",
    "        print(f\"Request {request_id}: Status {response.status_code}, Response: {response.json()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Request {request_id}: Failed with error: {e}\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Send multiple POST requests in parallel.\"\"\"\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        tasks = [send_request(client, i) for i in range(NUM_REQUESTS)]\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOCAL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.9 ms, sys: 10.2 ms, total: 21.1 ms\n",
      "Wall time: 230 ms\n",
      "{'prediction': [{'label': 'POSITIVE', 'score': 0.9997774958610535}]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = 'http://127.0.0.1:8000/infer'\n",
    "data = {'input': 'This is a positive input for the model.'}\n",
    "\n",
    "# Send POST request with JSON data\n",
    "%time response = requests.post(url, data=json.dumps(data), headers={'Content-Type': 'application/json'})\n",
    "\n",
    "\n",
    "# Print the response\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"prediction\":[{\"label\":\"POSITIVE\",\"score\":0.9997774958610535}]}'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997774958610535}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(response.text)['prediction']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local docker api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'Model Inference API'}\n"
     ]
    }
   ],
   "source": [
    "url = 'http://0.0.0.0:8000/'\n",
    "data = {'input': 'This is a positive input for the model.'}\n",
    "# Send POST request with JSON data\n",
    "#%time response = requests.post(url, data=json.dumps(data), headers={'Content-Type': 'application/json'})\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "# Print the response\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS docker api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "{\"detail\":[{\"type\":\"missing\",\"loc\":[\"body\",\"input\"],\"msg\":\"Field required\",\"input\":{\"text\":\"I love this product!\"}}]}\n",
      "Request failed with status code 422\n",
      "here\n",
      "{\"detail\":[{\"type\":\"missing\",\"loc\":[\"body\",\"input\"],\"msg\":\"Field required\",\"input\":{\"text\":\"This is the worst service ever.\"}}]}\n",
      "Request failed with status code 422\n",
      "here\n",
      "{\"detail\":[{\"type\":\"missing\",\"loc\":[\"body\",\"input\"],\"msg\":\"Field required\",\"input\":{\"text\":\"I'm feeling great today.\"}}]}\n",
      "Request failed with status code 422\n",
      "{\"detail\":[{\"type\":\"missing\",\"loc\":[\"body\",\"input\"],\"msg\":\"Field required\",\"input\":{\"text\":\"I am not happy with the results.\"}}]}\n",
      "Request failed with status code 422\n",
      "here\n",
      "{\"detail\":[{\"type\":\"missing\",\"loc\":[\"body\",\"input\"],\"msg\":\"Field required\",\"input\":{\"text\":\"Could be better, but I'm satisfied.\"}}]}\n",
      "Request failed with status code 422\n",
      "here\n",
      "{\"detail\":[{\"type\":\"missing\",\"loc\":[\"body\",\"input\"],\"msg\":\"Field required\",\"input\":{\"text\":\"Absolutely fantastic experience!\"}}]}\n",
      "Request failed with status code 422\n",
      "here\n",
      "{\"detail\":[{\"type\":\"missing\",\"loc\":[\"body\",\"input\"],\"msg\":\"Field required\",\"input\":{\"text\":\"The service was acceptable.\"}}]}\n",
      "Request failed with status code 422\n",
      "{\"detail\":[{\"type\":\"missing\",\"loc\":[\"body\",\"input\"],\"msg\":\"Field required\",\"input\":{\"text\":\"Exceeded all my expectations!\"}}]}\n",
      "Request failed with status code 422\n",
      "{\"detail\":[{\"type\":\"missing\",\"loc\":[\"body\",\"input\"],\"msg\":\"Field required\",\"input\":{\"text\":\"Not what I expected, quite disappointing.\"}}]}\n",
      "Request failed with status code 422\n",
      "{\"detail\":[{\"type\":\"missing\",\"loc\":[\"body\",\"input\"],\"msg\":\"Field required\",\"input\":{\"text\":\"I wouldn't recommend this to anyone.\"}}]}\n",
      "Request failed with status code 422\n",
      "Total time taken: 0.06 seconds\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "# Define the API endpoint\n",
    "url = 'http://0.0.0.0:8000/infer'\n",
    "\n",
    "# Sample texts for sentiment analysis\n",
    "texts = [\n",
    "    # Add a larger list of texts to better demonstrate concurrency\n",
    "    \"I love this product!\",\n",
    "    \"This is the worst service ever.\",\n",
    "    \"I'm feeling great today.\",\n",
    "    \"I am not happy with the results.\",\n",
    "    \"Could be better, but I'm satisfied.\",\n",
    "    \"Absolutely fantastic experience!\",\n",
    "    \"Not what I expected, quite disappointing.\",\n",
    "    \"The service was acceptable.\",\n",
    "    \"Exceeded all my expectations!\",\n",
    "    \"I wouldn't recommend this to anyone.\",\n",
    "    # Add more texts if desired\n",
    "]\n",
    "\n",
    "def send_request(text):\n",
    "    payload = {'text': text}\n",
    "    try:\n",
    "        print('here')\n",
    "        response = requests.post(url, json=payload)\n",
    "        print(response.text)\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            print(f\"Input Text: {text}\")\n",
    "            print(f\"Predicted Sentiment: {result['sentiment']}\")\n",
    "            print(f\"Confidence Score: {result['confidence']:.4f}\\n\")\n",
    "        else:\n",
    "            print(f\"Request failed with status code {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Number of threads to use\n",
    "num_threads = 5\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    futures = [executor.submit(send_request, text) for text in texts]\n",
    "    for future in as_completed(futures):\n",
    "        pass\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total time taken: {total_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 {'prediction': [{'label': 'POSITIVE', 'score': 0.9998855590820312}]}\n",
      "200 {'prediction': [{'label': 'POSITIVE', 'score': 0.9998855590820312}]}\n",
      "200 {'prediction': [{'label': 'POSITIVE', 'score': 0.9998855590820312}]}\n",
      "200 {'prediction': [{'label': 'POSITIVE', 'score': 0.9998855590820312}]}\n",
      "200 {'prediction': [{'label': 'POSITIVE', 'score': 0.9998855590820312}]}\n",
      "200 {'prediction': [{'label': 'POSITIVE', 'score': 0.9998855590820312}]}\n",
      "200 {'prediction': [{'label': 'POSITIVE', 'score': 0.9998855590820312}]}\n",
      "200 {'prediction': [{'label': 'POSITIVE', 'score': 0.9998855590820312}]}\n",
      "200 {'prediction': [{'label': 'POSITIVE', 'score': 0.9998855590820312}]}\n",
      "200 {'prediction': [{'label': 'POSITIVE', 'score': 0.9998855590820312}]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "url = \"http://localhost:8000/infer\"\n",
    "\n",
    "payload = {\"input\": \"I love this product!\"}\n",
    "\n",
    "# Function to make a POST request\n",
    "def make_request():\n",
    "    response = requests.post(url, json=payload)\n",
    "    print(response.status_code, response.json())\n",
    "\n",
    "# Send 10 parallel requests\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = [executor.submit(make_request) for _ in range(10)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 422\n",
      "Error: 422\n",
      "Error: 422\n",
      "Error: 422\n",
      "Error: 422\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Define the API endpoint and input data\n",
    "url = \"http://0.0.0.0:8000/infer\"\n",
    "texts = [\n",
    "    \"I love this product!\",\n",
    "    \"This is terrible.\",\n",
    "    \"Not sure about this one.\",\n",
    "    \"Could be better.\",\n",
    "    \"Absolutely amazing experience!\"\n",
    "]\n",
    "\n",
    "def send_request(text):\n",
    "    response = requests.post(url, json={\"text\": text})\n",
    "    if response.status_code == 200:\n",
    "        print(response.json())\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "\n",
    "# Send requests concurrently\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    executor.map(send_request, texts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
